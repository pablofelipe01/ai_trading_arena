"""
Chaos Testing for AI Trading Arena

Tests system resilience under adverse conditions:
- API failures
- Network errors
- Rate limiting
- Timeouts
- Concurrent stress

Usage:
    pytest tests/test_chaos.py -v
    pytest tests/test_chaos.py::test_api_failures -v
"""

import asyncio
import pytest
from unittest.mock import Mock, patch, AsyncMock

from models.llm_manager import LLMManager, ModelState
from core.arena_manager import ArenaManager
from tests.test_utils import (
    MockLLMClient,
    MockDataFetcher,
    create_mock_trading_decision,
    PerformanceMonitor
)
from utils.errors import LLMAPIError, LLMTimeoutError, LLMRateLimitError


# ============================================================================
# API Failure Tests
# ============================================================================


@pytest.mark.asyncio
async def test_single_model_api_failure():
    """Test system continues when one model fails"""
    manager = LLMManager()

    # Create mix of working and failing models
    working_client = MockLLMClient(
        provider="working",
        responses=[create_mock_trading_decision(action="BUY", confidence=0.8)]
    )

    failing_client = MockLLMClient(
        provider="failing",
        fail_count=999  # Always fails
    )

    # Add models
    manager.models = {
        "working": ModelState("working", 1, 100.0),
        "failing": ModelState("failing", 2, 100.0)
    }

    manager.models["working"].client = working_client
    manager.models["failing"].client = failing_client

    # Get decisions
    decisions = await manager.get_all_decisions(
        prompt="test prompt",
        symbol="BTC/USDT",
        current_price=100000.0
    )

    # Should have one success, one failure
    assert decisions["working"] is not None
    assert decisions["failing"] is None
    assert manager.models["failing"].errors > 0


@pytest.mark.asyncio
async def test_all_models_fail():
    """Test system handles all models failing"""
    manager = LLMManager()

    # All models fail
    for i in range(3):
        client = MockLLMClient(provider=f"model{i}", fail_count=999)
        state = ModelState(f"model{i}", i+1, 100.0)
        state.client = client
        manager.models[f"model{i}"] = state

    # Get decisions
    decisions = await manager.get_all_decisions(
        prompt="test prompt",
        symbol="BTC/USDT",
        current_price=100000.0
    )

    # All should fail
    assert all(d is None for d in decisions.values())
    assert all(state.errors > 0 for state in manager.models.values())


@pytest.mark.asyncio
async def test_intermittent_failures():
    """Test system handles intermittent failures"""
    # Fail first 2 calls, then succeed
    client = MockLLMClient(
        provider="intermittent",
        fail_count=2,
        responses=[create_mock_trading_decision(action="BUY")]
    )

    manager = LLMManager()
    state = ModelState("intermittent", 1, 100.0)
    state.client = client
    manager.models["intermittent"] = state

    # First two calls should fail
    decisions1 = await manager.get_all_decisions("test", "BTC/USDT", 100000.0)
    assert decisions1["intermittent"] is None

    decisions2 = await manager.get_all_decisions("test", "BTC/USDT", 100000.0)
    assert decisions2["intermittent"] is None

    # Third should succeed
    decisions3 = await manager.get_all_decisions("test", "BTC/USDT", 100000.0)
    assert decisions3["intermittent"] is not None
    assert decisions3["intermittent"].action == "BUY"


# ============================================================================
# Rate Limiting Tests
# ============================================================================


@pytest.mark.asyncio
async def test_rate_limit_handling():
    """Test system handles rate limiting correctly"""
    monitor = PerformanceMonitor()
    monitor.start()

    # Create slow clients (simulating rate limits)
    manager = LLMManager()

    for i in range(3):
        client = MockLLMClient(
            provider=f"slow{i}",
            delay_seconds=0.5,  # 500ms delay
            responses=[create_mock_trading_decision()]
        )
        state = ModelState(f"slow{i}", i+1, 100.0)
        state.client = client
        manager.models[f"slow{i}"] = state

    # Make multiple rapid calls
    for _ in range(3):
        start = asyncio.get_event_loop().time()
        await manager.get_all_decisions("test", "BTC/USDT", 100000.0)
        latency = asyncio.get_event_loop().time() - start
        monitor.record_api_call(latency)

    monitor.stop()
    report = monitor.get_report()

    # Should handle delays gracefully
    assert report["api_calls"] == 3
    assert report["avg_latency"] >= 0.5  # At least 500ms average


# ============================================================================
# Timeout Tests
# ============================================================================


@pytest.mark.asyncio
async def test_timeout_handling():
    """Test system handles timeouts"""
    # Create very slow client
    client = MockLLMClient(
        provider="timeout",
        delay_seconds=60.0  # Extremely slow
    )

    manager = LLMManager()
    state = ModelState("timeout", 1, 100.0)
    state.client = client
    manager.models["timeout"] = state

    # Should timeout quickly (within test timeout)
    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(
            manager.get_all_decisions("test", "BTC/USDT", 100000.0),
            timeout=2.0  # 2 second timeout
        )


# ============================================================================
# Concurrent Stress Tests
# ============================================================================


@pytest.mark.asyncio
async def test_concurrent_decision_requests():
    """Test system handles concurrent requests"""
    manager = LLMManager()

    # Add fast model
    client = MockLLMClient(
        provider="fast",
        delay_seconds=0.1,
        responses=[create_mock_trading_decision(action="BUY")]
    )
    state = ModelState("fast", 1, 100.0)
    state.client = client
    manager.models["fast"] = state

    # Make many concurrent requests
    tasks = [
        manager.get_all_decisions("test", "BTC/USDT", 100000.0)
        for _ in range(10)
    ]

    results = await asyncio.gather(*tasks)

    # All should succeed
    assert len(results) == 10
    assert all("fast" in r for r in results)
    assert all(r["fast"].action == "BUY" for r in results)


@pytest.mark.asyncio
async def test_memory_pressure():
    """Test system under memory pressure (many models)"""
    manager = LLMManager()

    # Create many models
    num_models = 20
    for i in range(num_models):
        client = MockLLMClient(
            provider=f"model{i}",
            responses=[create_mock_trading_decision()]
        )
        state = ModelState(f"model{i}", i+1, 100.0)
        state.client = client
        manager.models[f"model{i}"] = state

    # Get decisions from all
    decisions = await manager.get_all_decisions("test", "BTC/USDT", 100000.0)

    # Should handle all models
    assert len(decisions) == num_models
    assert all(d is not None for d in decisions.values())


# ============================================================================
# Network Error Simulation
# ============================================================================


@pytest.mark.asyncio
async def test_network_errors():
    """Test system handles network errors"""
    manager = LLMManager()

    # Simulate different network errors
    error_types = [
        ConnectionError("Connection refused"),
        TimeoutError("Request timeout"),
        Exception("DNS resolution failed")
    ]

    for i, error in enumerate(error_types):
        client = MockLLMClient(provider=f"network{i}")

        # Mock the API call to raise network error
        async def failing_api(*args, **kwargs):
            raise error

        client._call_api = failing_api

        state = ModelState(f"network{i}", i+1, 100.0)
        state.client = client
        manager.models[f"network{i}"] = state

    # Should handle all network errors
    decisions = await manager.get_all_decisions("test", "BTC/USDT", 100000.0)

    assert all(d is None for d in decisions.values())
    assert all(state.errors > 0 for state in manager.models.values())


# ============================================================================
# Data Corruption Tests
# ============================================================================


@pytest.mark.asyncio
async def test_corrupted_responses():
    """Test system handles corrupted API responses"""
    manager = LLMManager()

    # Mock client that returns invalid JSON
    client = MockLLMClient(provider="corrupt")

    async def corrupt_api(*args, **kwargs):
        return "{ invalid json }"

    client._call_api = corrupt_api

    state = ModelState("corrupt", 1, 100.0)
    state.client = client
    manager.models["corrupt"] = state

    # Should handle gracefully
    decisions = await manager.get_all_decisions("test", "BTC/USDT", 100000.0)

    assert decisions["corrupt"] is None
    assert state.errors > 0


# ============================================================================
# Recovery Tests
# ============================================================================


@pytest.mark.asyncio
async def test_error_recovery():
    """Test system recovers after errors"""
    # Model fails once, then recovers
    client = MockLLMClient(
        provider="recovery",
        fail_count=1,  # Fail only once
        responses=[create_mock_trading_decision(action="HOLD")]
    )

    manager = LLMManager()
    state = ModelState("recovery", 1, 100.0)
    state.client = client
    manager.models["recovery"] = state

    # First call fails
    decisions1 = await manager.get_all_decisions("test", "BTC/USDT", 100000.0)
    assert decisions1["recovery"] is None
    assert state.errors == 1

    # Second call succeeds
    decisions2 = await manager.get_all_decisions("test", "BTC/USDT", 100000.0)
    assert decisions2["recovery"] is not None
    assert state.errors == 1  # Error count stays same


# ============================================================================
# Performance Under Stress
# ============================================================================


@pytest.mark.asyncio
async def test_performance_degradation():
    """Test performance degrades gracefully under stress"""
    monitor = PerformanceMonitor()
    monitor.start()

    manager = LLMManager()

    # Add models with varying delays
    delays = [0.1, 0.3, 0.5, 0.7, 1.0]
    for i, delay in enumerate(delays):
        client = MockLLMClient(
            provider=f"delay{i}",
            delay_seconds=delay,
            responses=[create_mock_trading_decision()]
        )
        state = ModelState(f"delay{i}", i+1, 100.0)
        state.client = client
        manager.models[f"delay{i}"] = state

    # Run multiple rounds
    for _ in range(3):
        start = asyncio.get_event_loop().time()
        decisions = await manager.get_all_decisions("test", "BTC/USDT", 100000.0)
        latency = asyncio.get_event_loop().time() - start

        monitor.record_api_call(latency)

        # Should get all decisions despite delays
        assert len(decisions) == len(delays)

    monitor.stop()
    report = monitor.get_report()

    # Verify reasonable performance
    assert report["error_rate"] == 0.0
    assert report["avg_latency"] < 2.0  # Should be reasonably fast despite slowest being 1s


# ============================================================================
# Test Report
# ============================================================================


def test_chaos_report():
    """Generate chaos testing report"""
    print("\n" + "="*80)
    print("ðŸ”¥ CHAOS TESTING COMPLETE")
    print("="*80)
    print("\nTests Performed:")
    print("  âœ… API Failure Handling")
    print("  âœ… Rate Limit Management")
    print("  âœ… Timeout Handling")
    print("  âœ… Concurrent Request Stress")
    print("  âœ… Network Error Simulation")
    print("  âœ… Data Corruption Handling")
    print("  âœ… Error Recovery")
    print("  âœ… Performance Under Stress")
    print("\n" + "="*80)
    print("System Resilience: VERIFIED âœ…")
    print("="*80)


if __name__ == "__main__":
    # Run tests
    pytest.main([__file__, "-v", "--tb=short"])
